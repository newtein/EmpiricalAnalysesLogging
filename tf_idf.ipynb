{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lxml import etree\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import math\n",
    "import csv\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "# Provide path to tagfile, SO data-folder and output_location\n",
    "datamap=OrderedDict()\n",
    "datamap[\"AE\"]=\"C:/Users/harshitgujral/Desktop/Stack/data/android.stackexchange.com/Posts.xml\"\n",
    "datamap[\"DB\"]=\"C:/Users/harshitgujral/Desktop/Stack/data/dba.stackexchange.com/Posts.xml\"\n",
    "datamap[\"SO\"]=\"C:/Users/harshitgujral/Desktop/Stack/data/stackoverflow.com-Posts/Posts.xml\"\n",
    "\n",
    "datamap[\"SE\"]=\"C:/Users/harshitgujral/Desktop/Stack/data/softwareengineering.stackexchange.com/Posts.xml\"\n",
    "datamap[\"SF\"]=\"C:/Users/harshitgujral/Desktop/Stack/data/serverfault.com/Posts.xml\"\n",
    "datamap[\"SU\"]=\"C:/Users/harshitgujral/Desktop/Stack/data/superuser.com/Posts.xml\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prog_lang(filename):\n",
    "    data=OrderedDict()\n",
    "    f=open(filename,\"r\")\n",
    "    for l in f:\n",
    "        row=l.split(\"\\t\")\n",
    "        key=row[0].lower().strip()\n",
    "        data[key]=[]\n",
    "        for element in row[1].split(\",\"):\n",
    "            data[key].append(element.strip().lower())\n",
    "    return data\n",
    "\n",
    "def atleast_one(a, b):\n",
    "    return not set(a).isdisjoint(b)\n",
    "\n",
    "def refine_tags(tags):\n",
    "    if tags!=None:\n",
    "        l=tags.split('><')\n",
    "        l=[i.replace('>','').replace('<','').lower() for i in l]\n",
    "        return l\n",
    "    else:\n",
    "        return [\"???\"]\n",
    "    \n",
    "def read_tags(filename):\n",
    "    logtags=[]\n",
    "    fn=open(filename,\"r\")\n",
    "    next(fn)   # SKipping the first line\n",
    "    for row in fn:\n",
    "        logtags.append(row.split(\",\")[0].strip().lower())   \n",
    "    return logtags\n",
    "\n",
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagfile=\"tags/Logging-topic-modelling - Sheet3.csv\"\n",
    "output_location=\"output/\"\n",
    "\n",
    "prog_tags=read_prog_lang(\"tags/logging-programming - Sheet1.tsv\")\n",
    "general_tags=list(set(read_tags(tagfile)))\n",
    "for i in prog_tags:\n",
    "    general_tags.extend(prog_tags[i])\n",
    "general_tags=list(set(general_tags))\n",
    "langs=[i for i in prog_tags.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['java', 'c', 'c++', 'python', 'c#', 'javascript']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs.remove('android')\n",
    "langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_lookup = {}\n",
    "for lang in langs:\n",
    "    path = \"programming_language/{}\".format(lang)\n",
    "    path_lookup[lang] = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_set(path):\n",
    "    word_set = set()\n",
    "    tokenized_document = {}\n",
    "    tokenized_document_set = {}\n",
    "    documents = os.listdir(path)\n",
    "    for document_name in documents:\n",
    "        f = open(\"{}/{}\".format(path, document_name), \"r\")\n",
    "        document = f.read().lower()\n",
    "        tokens = [i for i in word_tokenize(document) if i not in stop_words and not i.isdigit()]\n",
    "        tokenized_document[document_name] = tokens\n",
    "        tokenized_document_set[document_name] = set(tokens)\n",
    "        f.close()\n",
    "        for word in tokens:\n",
    "            if word not in stop_words and not word.isdigit():\n",
    "                word_set.add(word) \n",
    "    return tokenized_document, tokenized_document_set, word_set, len(documents)\n",
    "\n",
    "def check_times_exists_in_documents(tokenized_document_set, word):\n",
    "    df = 0\n",
    "    for document_name in tokenized_document_set:\n",
    "        if word in tokenized_document_set[document_name]:\n",
    "            df+=1\n",
    "    return df\n",
    "\n",
    "def write_to_csv(filename, header, rows):\n",
    "    f= open(filename, \"w\", newline = '')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF created for java\n",
      "results created for java\n",
      "DF created for c\n",
      "results created for c\n",
      "DF created for c++\n",
      "results created for c++\n",
      "DF created for python\n",
      "results created for python\n",
      "DF created for c#\n",
      "results created for c#\n",
      "DF created for javascript\n",
      "results created for javascript\n"
     ]
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    path = path_lookup[lang]\n",
    "    tokenized_document, tokenized_document_set, word_set, N = make_word_set(path)\n",
    "\n",
    "    DF = {}\n",
    "    for word in word_set:\n",
    "        DF[word] = check_times_exists_in_documents(tokenized_document_set, word)\n",
    "    print(\"DF created for {}\".format(lang))\n",
    "    result_header = ['Document name', 'Term','Term frequency (TF)', 'Document frequency (DF)','Number of documents (N)',\n",
    "                     'Number of times term occurred in the document', 'Number of words in the document','TF-IDF-Score']\n",
    "    results = []\n",
    "    for document_name in tokenized_document:\n",
    "        document = tokenized_document[document_name]\n",
    "        document_set = tokenized_document_set[document_name]\n",
    "        number_of_words = len(document)\n",
    "        for word in word_set:\n",
    "            if word in document_set:\n",
    "                number_of_times = document.count(word)\n",
    "                tf = number_of_times/number_of_words\n",
    "                df = DF[word]\n",
    "                tf_idf = tf*math.log(N/df)\n",
    "                results.append([document_name, word, tf, df, N,number_of_times,number_of_words,  tf_idf])  \n",
    "    print(\"results created for {}\".format(lang))\n",
    "    filename=\"{}/{}.csv\".format(\"tf_idf\", lang)            \n",
    "    write_to_csv(filename, result_header, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
